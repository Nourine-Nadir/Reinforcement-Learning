{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# →Q-Learning\n",
    "\n",
    "It is temporal difference learning on $Q$-function\n",
    "\n",
    "$Q^{new}(s_k,a_k)=Q^{old}(s_k,a_k)+\\alpha(r_k+\\gamma \\max_a Q(s_{k+1},a)-Q^{old}(s_k,a_k))$ \n",
    "\n",
    "Off policy TD(0) learning of the quality function Q\n",
    "\n",
    "What we mean by **Off policy** is that we can take **sub-optimal** $a_k$ actions to get the reward but still **maximize** the next action in $s_{k+1}$ though, this helps to learn even when **not taking best** $a_k$ actions.\n",
    "\n",
    "The Off policy can be **confusing** since we are saying that we can take **sub-optimal** actions but there is that **term** in the update function: $max_a Q(s_{k+1},a)$\n",
    "\n",
    "**Many** policies are used in **experiments** and at the **experience replay** step we iterate through actions even if they are sub-optimal but we **assume** that the **best** actions will be taken in next steps. This is done by replaying experiments done **by us** or **importing** others and learn from them; this ensure treating **different** policies.\n",
    "\n",
    "**Exploration vs. exploitation: $\\epsilon$-greedy actions**\n",
    "\n",
    "**Random** exploration element is introduced to $Q$-learning, the popular technique is the  **$\\epsilon$-greedy.** Taking the action $a_k$ will be taken based on the current $Q$ function, with a probability $1-\\epsilon$, where $\\epsilon \\in[0,1]$. for example $\\epsilon=0.05$ there will be a 95% **probability** of taking best action and 5% **chance** of exploring a sub-optimal one. \n",
    "\n",
    "This epsilon value can be decayed as we iterate to go more **On-Policy** once we learned a good $Q$-function.\n",
    "\n",
    "$Q$ -learning applies to **discrete** action spaces $A$ and state spaces $S$ governed by a **finite** MDP. A table of $Q$ values is used to represent the $Q$ function, and thus it doesn’t **scale** well to **large** state spaces. Typically function **approximation** is used to represent the $Q$ function, such as a **neural network** in deep $Q$-learning.\n",
    "\n",
    "> Because $Q$-learning is off-policy, it is possible to learn from action-state sequences that do not use the current optimal policy. For example, it is possible to store past experiences, such as previously played games, and replay these experiences to further improve the Q function.\n",
    ">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d539f2a704def688"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.804438900Z",
     "start_time": "2024-07-20T11:30:42.782887800Z"
    }
   },
   "id": "initial_id",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LunarLanderAgent:\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 initial_epsilon: float,\n",
    "                 epsilon_decay: float,\n",
    "                 final_epsilon: float,\n",
    "                 discount_factor: float = 0.95,\n",
    "                 discrete_actions: int = 4):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.discrete_actions = discrete_actions\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.discrete_actions))\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        # Round each value in the state to 1 decimal place\n",
    "        # Convert to tuple for hashability\n",
    "       \n",
    "        rounded_state = np.round(state, 1)  # Slice to exclude last element\n",
    "        # Append the original terminated flag (boolean)\n",
    "        return tuple(np.append(rounded_state, state[-1])) \n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.discrete_actions)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[discretized_state]))\n",
    "    \n",
    "    def update_q_values(self, state, action, reward, terminated, next_state):\n",
    "        state = self.discretize_state(state)\n",
    "        next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        if not terminated:          \n",
    "            future_q_value = np.max(self.q_values[next_state])\n",
    "        else:\n",
    "            future_q_value = 0\n",
    "        temporal_difference = (reward + (self.discount_factor * future_q_value)) - self.q_values[state][action]\n",
    "        self.q_values[state][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.833514800Z",
     "start_time": "2024-07-20T11:30:42.822002300Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_td_n(agent, env, n_steps, n_episodes):\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state = agent.discretize_state(state)#\n",
    "            next_state = agent.discretize_state(next_state)\n",
    "            if len(rewards) == n_steps or done:\n",
    "                return_sum = sum([agent.discount_factor**i * r for i, r in enumerate(rewards)])\n",
    "                if not done:\n",
    "                    return_sum += agent.discount_factor**n_steps * np.max(agent.q_values[next_state])\n",
    "                \n",
    "                for i in range(len(rewards)):\n",
    "                    G = sum([agent.discount_factor**(j-i) * r for j, r in enumerate(rewards[i:])])\n",
    "                    if i + len(rewards) - 1 < n_steps and not done:\n",
    "                        G += agent.discount_factor**(len(rewards)-i) * np.max(agent.q_values[next_state])\n",
    "                    agent.update_q_values(states[i], actions[i], G, terminated, next_state)\n",
    "\n",
    "                rewards.pop(0)\n",
    "                states.pop(0)\n",
    "                actions.pop(0)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.851547300Z",
     "start_time": "2024-07-20T11:30:42.839515500Z"
    }
   },
   "id": "104aba317b9b8593",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "n_episodes = 100_00\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.999\n",
    "final_epsilon = 0.05\n",
    "\n",
    "agent = LunarLanderAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.875009200Z",
     "start_time": "2024-07-20T11:30:42.855549300Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.876009500Z",
     "start_time": "2024-07-20T11:30:42.871071800Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=60)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:30:42.889519900Z",
     "start_time": "2024-07-20T11:30:42.887520400Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:05<00:00, 32.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_td_n(agent, env, n_steps=10, n_episodes=n_episodes, )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:35:47.975423900Z",
     "start_time": "2024-07-20T11:30:42.902036700Z"
    }
   },
   "id": "a0979b1986430066",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "q_values = np.array([value for key, value in agent.q_values.items()])\n",
    "print(np.argmax(q_values,axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:35:48.035431400Z",
     "start_time": "2024-07-20T11:35:47.987875600Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:  obs = [ 0.24714136  0.03138035 -0.75699633 -0.7195032   0.08950049  4.836358\n",
      "  1.          0.        ] , reward = -100\n",
      "step 1:  obs = [ 0.8094761  -0.26926488  1.1745639  -0.5409227  -1.1714681  -6.990555\n",
      "  1.          0.        ] , reward = -100\n",
      "step 2:  obs = [-0.3463716   0.0411521  -0.34708774 -1.7024463   0.18780096 -0.13177453\n",
      "  0.          1.        ] , reward = 10.758509601442512\n",
      "step 2:  obs = [-0.34415117  0.01864594  0.85374004 -0.52685505  0.06931063 -3.2182388\n",
      "  0.          1.        ] , reward = -100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"LunarLander-v2\",render_mode='rgb_array')\n",
    "# env = CustomRewardWrapper(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "img = ax.imshow(env.render())\n",
    "actions = ['Move Up','Move Right','Move Down','Move Left']\n",
    "rewards = 0\n",
    "num_epochs= 3\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if reward >10 : \n",
    "            print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        if done: \n",
    "            print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:36:04.934614400Z",
     "start_time": "2024-07-20T11:35:48.064868800Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean episode rewards = -164.13144447604887\n"
     ]
    }
   ],
   "source": [
    "print(f'mean episode rewards = {rewards/num_epochs}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:36:04.954383900Z",
     "start_time": "2024-07-20T11:36:04.947383300Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space shape : 4\n",
      "observation space shape : Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LunarLander' object has no attribute 'nS'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction space shape : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# Number of possible actions is 4\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobservation space shape : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobservation space numbers : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv\u001B[38;5;241m.\u001B[39mnS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#-------------- obesrvation is a tupe of 3 values : --------------\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#1) player cards value\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#2) dealer's face up card\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m#3) usable ace for player, equal 1 if ace is considered an 11 without busting\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward range : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv\u001B[38;5;241m.\u001B[39mreward_range\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# default reward range is set to -inf +inf\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\core.py:315\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    311\u001B[0m logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto get this variable you can do `env.unwrapped.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)` that will search the reminding wrappers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    314\u001B[0m )\n\u001B[1;32m--> 315\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\core.py:315\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    311\u001B[0m logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto get this variable you can do `env.unwrapped.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)` that will search the reminding wrappers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    314\u001B[0m )\n\u001B[1;32m--> 315\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\core.py:315\u001B[0m, in \u001B[0;36mWrapper.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    310\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccessing private attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is prohibited\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    311\u001B[0m logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menv.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto get this variable you can do `env.unwrapped.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)` that will search the reminding wrappers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    314\u001B[0m )\n\u001B[1;32m--> 315\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'LunarLander' object has no attribute 'nS'"
     ]
    }
   ],
   "source": [
    "print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "print(f'observation space shape : {env.observation_space}') \n",
    "print(f'observation space numbers : {env.nS}') \n",
    "#-------------- obesrvation is a tupe of 3 values : --------------\n",
    "#1) player cards value\n",
    "#2) dealer's face up card\n",
    "#3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "\n",
    "print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# print(f'\\nEnv specs : {env.spec}') \n",
    "print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:36:04.991854500Z",
     "start_time": "2024-07-20T11:36:04.962448Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
