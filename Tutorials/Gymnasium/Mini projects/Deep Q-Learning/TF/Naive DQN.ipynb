{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:34:04.233402600Z",
     "start_time": "2024-07-28T02:34:04.218484100Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:34:04.258730500Z",
     "start_time": "2024-07-28T02:34:04.231310500Z"
    }
   },
   "id": "948a80f02f2787d1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = layers.Dense(128, activation='relu', input_shape=input_dims)\n",
    "        self.fc2 = layers.Dense( n_actions, activation=None)\n",
    "        self.flatten = layers.Flatten() \n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss = keras.losses.MeanSquaredError()\n",
    "    \n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        with tf.device('/GPU:0'):  \n",
    "\n",
    "            x = self.fc1(inputs)\n",
    "            x = self.fc2(x)\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:34:04.264249400Z",
     "start_time": "2024-07-28T02:34:04.247238700Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 n_actions,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.0001,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=1e-5,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = tf.convert_to_tensor([obs], dtype=tf.float32)\n",
    "                actions = self.Q(state)\n",
    "                actions = tf.squeeze(actions)\n",
    "                action = tf.argmax(actions)\n",
    "                # print('All Actions: ', actions)\n",
    "                # print('Selected Action: ', action.numpy())\n",
    "                action = int(action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor([action], dtype=tf.int32)\n",
    "            rewards = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "            \n",
    "            # print(f'states {states.shape}, next_states {next_states.shape}')\n",
    "            q_pred = self.Q.call(states)\n",
    "#             print(f'q_pred: {q_pred.shape}')\n",
    "            q_next = tf.reduce_max(self.Q.call(next_states))\n",
    "            q_next_max = tf.reduce_max(q_next)\n",
    "            # print(f'q_next_max: {q_next_max.shape}')        \n",
    "            q_target = rewards + self.gamma * q_next_max\n",
    "            # print(f'q_target: {q_target.shape}')\n",
    "            loss = self.Q.loss(q_pred, q_target)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.Q.trainable_weights)\n",
    "        self.Q.optimizer.apply_gradients(zip(gradients, self.Q.trainable_weights))\n",
    "        self.decrement_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:34:04.279447500Z",
     "start_time": "2024-07-28T02:34:04.268250400Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score 19.0 avg score 19.0 epsilon 1.000\n",
      "Episode 100 score 22.0 avg score 21.1 epsilon 0.979\n",
      "Episode 200 score 16.0 avg score 22.3 epsilon 0.956\n",
      "Episode 300 score 19.0 avg score 22.6 epsilon 0.934\n",
      "Episode 400 score 19.0 avg score 21.8 epsilon 0.912\n",
      "Episode 500 score 15.0 avg score 22.0 epsilon 0.890\n",
      "Episode 600 score 15.0 avg score 21.2 epsilon 0.869\n",
      "Episode 700 score 12.0 avg score 22.0 epsilon 0.847\n",
      "Episode 800 score 12.0 avg score 24.6 epsilon 0.822\n",
      "Episode 900 score 17.0 avg score 22.2 epsilon 0.800\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    nb_episodes = 1000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    agent = Agent(input_dims= env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  )\n",
    "    \n",
    "    for i in range (nb_episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs, _ = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            next_obs, reward, terminated,truncated, _ = env.step(action)\n",
    "            score+= reward\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            done = truncated or terminated\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.epsilon) )\n",
    "    \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:39:04.406638800Z",
     "start_time": "2024-07-28T02:34:04.276399100Z"
    }
   },
   "id": "a7cbe0fb29d7d5fc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['Left','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 10\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:39:16.257356100Z",
     "start_time": "2024-07-28T02:39:04.409640400Z"
    }
   },
   "id": "bcf0b9dfbeacb01c",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x21183c89070>]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:39:16.351469Z",
     "start_time": "2024-07-28T02:39:16.257356100Z"
    }
   },
   "id": "7ec8c5d01b740702",
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
