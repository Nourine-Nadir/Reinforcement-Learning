{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:08.224134500Z",
     "start_time": "2024-07-27T13:15:08.196721Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:08.243174600Z",
     "start_time": "2024-07-27T13:15:08.228135300Z"
    }
   },
   "id": "948a80f02f2787d1",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = layers.Dense(128, activation='relu', input_shape=input_dims)\n",
    "        self.fc2 = layers.Dense( n_actions, activation=None)\n",
    "        \n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss = keras.losses.MeanSquaredError()\n",
    "    \n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        with tf.device('/GPU:0'):  \n",
    "\n",
    "            x = self.fc1(inputs)\n",
    "            x = self.fc2(x)\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:08.287562100Z",
     "start_time": "2024-07-27T13:15:08.248174400Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims,\n",
    "                 gamma=0.99,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=0.99,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        with tf.device('/GPU:0'):  # or '/CPU:0' if you're using CPU\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = tf.convert_to_tensor([obs], dtype=tf.float32)\n",
    "                actions = self.Q(state)\n",
    "                actions = tf.squeeze(actions)\n",
    "                action = tf.argmax(actions)\n",
    "                print('All Actions: ', actions)\n",
    "                print('Selected Action: ', action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay \\\n",
    "                       if self.epsilon > self.final_epsilon else self.final_epsilon\n",
    "        \n",
    "    @tf.function\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor([action], dtype=tf.int32)\n",
    "            rewards = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "            \n",
    "            q_pred = tf.gather_nd(self.Q(states), tf.expand_dims(actions, -1))\n",
    "            print(f'q_pred: {q_pred.shape}')\n",
    "            q_next = tf.reduce_max(self.Q(next_states), axis=1)\n",
    "            print(f'q_next: {q_next.shape}')        \n",
    "            q_target = rewards + self.gamma * q_next \n",
    "            print(f'q_target: {q_target.shape}')\n",
    "        \n",
    "            loss = self.Q.loss(q_pred, q_target)\n",
    "        gradients = tape.gradient(loss, self.Q.trainable_variables)\n",
    "        self.Q.optimizer.apply_gradients(zip(gradients, self.Q.trainable_variables))\n",
    "        self.decrement_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:08.320202600Z",
     "start_time": "2024-07-27T13:15:08.293090400Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nadir= Agent(lr=0.1, n_actions=4, input_dims=[128,128,3])\n",
    "nadir.choose_action([12,12,2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:08.360783900Z",
     "start_time": "2024-07-27T13:15:08.319201900Z"
    }
   },
   "id": "abe8e6d09e5c53d0",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_pred: (1, 128, 128, 4)\n",
      "q_next: (1, 128, 4)\n",
      "q_target: (1, 128, 4)\n",
      "trainable weights : [<tf.Variable 'linear_deep_q_network_2/dense_4/kernel:0' shape=(3, 128) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_4/bias:0' shape=(128,) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_5/kernel:0' shape=(128, 4) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_5/bias:0' shape=(4,) dtype=float32>]\n",
      "q_pred: (1, 128, 128, 4)\n",
      "q_next: (1, 128, 4)\n",
      "q_target: (1, 128, 4)\n",
      "trainable weights : [<tf.Variable 'linear_deep_q_network_2/dense_4/kernel:0' shape=(3, 128) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_4/bias:0' shape=(128,) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_5/kernel:0' shape=(128, 4) dtype=float32>, <tf.Variable 'linear_deep_q_network_2/dense_5/bias:0' shape=(4,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "nadir.learn(np.random.random((128,128,3)),2,2,np.random.random((128,128,3))) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:15:11.492071500Z",
     "start_time": "2024-07-27T13:15:08.352266900Z"
    }
   },
   "id": "26798ba71a6b2eb2",
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
