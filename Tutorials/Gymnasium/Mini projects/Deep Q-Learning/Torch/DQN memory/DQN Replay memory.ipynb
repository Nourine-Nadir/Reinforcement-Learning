{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Q learning for continiuous spaces\n",
    "In continuous space and in the case of this notebook, It is an image where occurs some changes by the agent or the environment itself. Treating this with a naive method won't get us a good result since a state at time t isn't relevant to estimate its reward. The solution is to get more than one state chronologically to estimate the reward of an action, then we need to store our actions and states in a memory to practice that."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c942c44562cad2ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import main libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb7818f37b1eb679"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T18:51:32.126515200Z",
     "start_time": "2024-08-08T18:51:32.105007Z"
    }
   },
   "id": "initial_id",
   "execution_count": 245
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeepQNetwork"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5a706fe2ab3af5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 input_dims,\n",
    "                 fc1_dims,\n",
    "                 fc2_dims,\n",
    "                 fc3_dims,\n",
    "                 nb_actions):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.fc3_dims = fc2_dims\n",
    "        self.nb_actions = nb_actions\n",
    "        \n",
    "        print(f'input {self.input_dims} nb_actions {nb_actions}')\n",
    "        self.fc1= nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2= nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.fc3_dims)\n",
    "        self.fc4 = nn.Linear(self.fc3_dims, self.nb_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss = nn.SmoothL1Loss()  # Use Huber loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        l1 = F.relu(self.fc1(state))\n",
    "        l2 = F.relu(self.fc2(l1))\n",
    "        l3 = F.relu(self.fc3(l2))\n",
    "        actions = self.fc4(l3) # don't activate the last layer\n",
    "        \n",
    "        return actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T18:51:32.147519900Z",
     "start_time": "2024-08-08T18:51:32.133516900Z"
    }
   },
   "id": "7cc06d681b644d92",
   "execution_count": 246
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "968a96ae49428a44"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 batch_size:float,\n",
    "                 input_dims,\n",
    "                 n_actions:int,\n",
    "                 eps_decay:float,\n",
    "                 max_mem_size:int = 10_000,\n",
    "                 gamma:float=0.99,\n",
    "                 initial_eps:float=1.0,\n",
    "                 final_eps:float=0.01,\n",
    "                 lr:float=1e-4):\n",
    "        self.gamma= gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dims = input_dims\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.eps_decay = eps_decay\n",
    "        self.lr = lr\n",
    "        self.eps = initial_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.final_eps = final_eps\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_counter = 0        \n",
    "        \n",
    "        self.Q_eval = DQNetwork(lr=self.lr,\n",
    "                                input_dims=self.input_dims,\n",
    "                                nb_actions=n_actions,\n",
    "                                fc1_dims=256, fc2_dims=256, fc3_dims=128)\n",
    "        self.state_memory = np.zeros((self.mem_size, *self.input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *self.input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size,dtype=np.int32) # set of integers because our actions belong to a discrete space\n",
    "        self.reward_memory = np.zeros(self.mem_size,dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size,dtype=bool)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done): #sate_ : new state\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    def choose_action(self, obs ):\n",
    "        if np.random.random() > self.eps :\n",
    "            state = T.tensor(np.array([obs])).to(self.Q_eval.device) # we use the brackets because of the way the DQN is set up\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_size, self.mem_counter)  \n",
    "        batch = np.random.choice(max_mem, size=self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]# get the values of the actions we took\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch]= 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "       \n",
    "        self.eps = max((self.eps - self.eps_decay), self.final_eps)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T18:51:32.172525600Z",
     "start_time": "2024-08-08T18:51:32.166524500Z"
    }
   },
   "id": "46b1f3bbfdd1c83",
   "execution_count": 247
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from DQN_q_eval import Agent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T18:51:32.189529200Z",
     "start_time": "2024-08-08T18:51:32.180527800Z"
    }
   },
   "id": "c3bbdc3356fb0b0b",
   "execution_count": 248
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.999\n",
      "Episode 0 score -96.4 avg score -96.4 epsilon 1.000\n",
      "0.908240850000413\n",
      "Episode 100 score -264.8 avg score -170.2 epsilon 0.909\n",
      "0.8034557400008899\n",
      "Episode 200 score -110.2 avg score -151.8 epsilon 0.804\n",
      "0.6902390700014052\n",
      "Episode 300 score -89.8 avg score -132.6 epsilon 0.691\n",
      "0.5302192500021334\n",
      "Episode 400 score -95.3 avg score -106.3 epsilon 0.531\n",
      "0.17666316000195037\n",
      "Episode 500 score -49.6 avg score -126.7 epsilon 0.177\n",
      "0.04995\n",
      "Episode 600 score -47.3 avg score -229.3 epsilon 0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[249], line 28\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# if avg_score >-40 :\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#     j=i\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#     break\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[1;32m---> 28\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m     obs_, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     30\u001B[0m     score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement Learning\\Tutorials\\Gymnasium\\Mini projects\\Deep Q-Learning\\Torch\\DQN memory\\DQN_q_eval.py:92\u001B[0m, in \u001B[0;36mAgent.choose_action\u001B[1;34m(self, obs)\u001B[0m\n\u001B[0;32m     89\u001B[0m     state \u001B[38;5;241m=\u001B[39m T\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39marray([obs]))\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m     90\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQ_eval\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# we use the brackets because of the way the DQN is set up\u001B[39;00m\n\u001B[0;32m     91\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQ_eval\u001B[38;5;241m.\u001B[39mforward(state)\n\u001B[1;32m---> 92\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     94\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from utils import plotLearning\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    print(env.observation_space.shape[0])\n",
    "    agent = Agent(gamma=.99,\n",
    "                  initial_eps=1.0,\n",
    "                  eps_decay=1e-5,\n",
    "                  final_eps=0.05,\n",
    "                  batch_size=128,\n",
    "                  n_actions=4,\n",
    "                  input_dims=[8],\n",
    "                  lr=0.0001,\n",
    "                  max_mem_size=10_000)\n",
    "    scores, eps_history = [],[]\n",
    "    n_episodes = 2000\n",
    "    j= n_episodes\n",
    "    avg_score = -9999\n",
    "    for i in range(n_episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        # if avg_score >-40 :\n",
    "        #     j=i\n",
    "        #     break\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs_, reward, terminated, truncated, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(obs, action, reward, obs_, done)\n",
    "            agent.learn()\n",
    "            obs = obs_\n",
    "            done = terminated or truncated\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.eps)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "                avg_score = np.mean(scores[-100:]) \n",
    "                print(agent.eps*.999)\n",
    "                print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.eps) )\n",
    "    x = [i+1 for i in range(j)]\n",
    "    filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores,eps_history, filename)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T19:00:10.741253900Z",
     "start_time": "2024-08-08T18:51:32.202534Z"
    }
   },
   "id": "cf1c9f9629526b19",
   "execution_count": 249
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# x = [i+1 for i in range(4426)]\n",
    "# filename = 'lunar_lander.png'\n",
    "# plotLearning(x, scores,eps_history, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T19:00:10.753257300Z",
     "start_time": "2024-08-08T19:00:10.752257Z"
    }
   },
   "id": "49a918586e8b00a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('LunarLander-v2', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['nothing','Left','main','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 1000\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        print(reward)\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T19:00:10.765259100Z",
     "start_time": "2024-08-08T19:00:10.765259100Z"
    }
   },
   "id": "73f443e038177892",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
