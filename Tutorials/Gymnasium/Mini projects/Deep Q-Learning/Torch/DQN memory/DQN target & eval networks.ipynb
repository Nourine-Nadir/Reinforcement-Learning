{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Q learning for continiuous spaces\n",
    "In continuous space and in the case of this notebook, It is an image where occurs some changes by the agent or the environment itself. Treating this with a naive method won't get us a good result since a state at time t isn't relevant to estimate its reward. The solution is to get more than one state chronologically to estimate the reward of an action, then we need to store our actions and states in a memory to practice that."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c942c44562cad2ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import main libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb7818f37b1eb679"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:09:28.568417400Z",
     "start_time": "2024-08-09T01:09:26.296092800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeepQNetwork"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5a706fe2ab3af5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 input_dims,\n",
    "                 fc1_dims,\n",
    "                 fc2_dims,\n",
    "                 fc3_dims,\n",
    "                 nb_actions):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.fc3_dims = fc2_dims\n",
    "        self.nb_actions = nb_actions\n",
    "        \n",
    "\n",
    "        self.fc1= nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2= nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.fc3_dims)\n",
    "        self.fc4 = nn.Linear(self.fc3_dims, self.nb_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss = nn.SmoothL1Loss()  # Use Huber loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        l1 = F.relu(self.fc1(state))\n",
    "        l2 = F.relu(self.fc2(l1))\n",
    "        l3 = F.relu(self.fc3(l2))\n",
    "        actions = self.fc4(l3) # don't activate the last layer\n",
    "        \n",
    "        return actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:09:28.585451200Z",
     "start_time": "2024-08-09T01:09:28.575927300Z"
    }
   },
   "id": "7cc06d681b644d92",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "968a96ae49428a44"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 batch_size:float,\n",
    "                 input_dims,\n",
    "                 n_actions:int,\n",
    "                 max_mem_size:int = 10_000,\n",
    "                 gamma:float=0.99,\n",
    "                 initial_eps:float=1.0,\n",
    "                 final_eps:float=0.01,\n",
    "                 eps_decay:float=1e-4,\n",
    "                 lr:float=1e-4):\n",
    "        self.gamma= gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dims = input_dims\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.eps_decay = eps_decay\n",
    "        self.lr = lr\n",
    "        self.eps = initial_eps\n",
    "        self.eps_decay = eps_decay\n",
    "        self.final_eps = final_eps\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_counter = 0        \n",
    "        self.sequence_length = 32  # You can adjust this value\n",
    "\n",
    "        self.target_update_frequency = 100\n",
    "        self.learn_step_counter = 0\n",
    "        self.Q_eval = DQNetwork(lr=self.lr,\n",
    "                                input_dims=self.input_dims,\n",
    "                                nb_actions=n_actions,\n",
    "                                fc1_dims=256, fc2_dims=128, fc3_dims=64)\n",
    "        self.Q_target =  DQNetwork(lr=self.lr,\n",
    "                                input_dims=self.input_dims,\n",
    "                                nb_actions=n_actions,\n",
    "                                fc1_dims=256, fc2_dims=128, fc3_dims=64)\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *self.input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *self.input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size,dtype=np.int32) # set of integers because our actions belong to a discrete space\n",
    "        self.reward_memory = np.zeros(self.mem_size,dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size,dtype=bool)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done): #sate_ : new state\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "        \n",
    "\n",
    "        \n",
    "    def choose_action(self, obs ):\n",
    "        if np.random.random() > self.eps :\n",
    "            self.Q_eval.eval()  # Set to evaluation mode\n",
    "            with T.no_grad():\n",
    "                state = T.tensor(np.array([obs])).to(self.Q_eval.device)\n",
    "                actions = self.Q_eval.forward(state)\n",
    "            self.Q_eval.train()  # Set back to training mode\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_size, self.mem_counter)  \n",
    "        batch = np.random.choice(max_mem, size=self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        self.Q_target.eval()  # Set to evaluation mode\n",
    "        with T.no_grad():\n",
    "            q_next = self.Q_target.forward(new_state_batch)\n",
    "        self.Q_target.train()  # Set back to training mode\n",
    "     \n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.target_update_frequency == 0:\n",
    "            self.Q_target.load_state_dict(self.Q_eval.state_dict())\n",
    "        \n",
    "        self.eps = max((self.eps - self.eps_decay), self.final_eps)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:09:28.615867200Z",
     "start_time": "2024-08-09T01:09:28.597411500Z"
    }
   },
   "id": "46b1f3bbfdd1c83",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Episode 0 score -178.7 avg score -178.7 epsilon 1.000\n",
      "Episode 100 score -106.2 avg score -136.3 epsilon 0.815\n",
      "Episode 200 score -85.8 avg score -86.6 epsilon 0.614\n",
      "Episode 300 score -16.0 avg score -32.6 epsilon 0.050\n",
      "Episode 400 score -11.3 avg score 135.9 epsilon 0.050\n",
      "Episode 500 score 277.4 avg score 211.8 epsilon 0.050\n",
      "Episode 600 score 261.6 avg score 28.4 epsilon 0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     32\u001B[0m agent\u001B[38;5;241m.\u001B[39mstore_transition(obs, action, reward, obs_, done)\n\u001B[1;32m---> 33\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m obs \u001B[38;5;241m=\u001B[39m obs_\n\u001B[0;32m     35\u001B[0m done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "Cell \u001B[1;32mIn[3], line 88\u001B[0m, in \u001B[0;36mAgent.learn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQ_target\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# Set to evaluation mode\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m T\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 88\u001B[0m     q_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mQ_target\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_state_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mQ_target\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Set back to training mode\u001B[39;00m\n\u001B[0;32m     91\u001B[0m q_next[terminal_batch] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "Cell \u001B[1;32mIn[2], line 29\u001B[0m, in \u001B[0;36mDQNetwork.forward\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,state):\n\u001B[1;32m---> 29\u001B[0m     l1 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     30\u001B[0m     l2 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(l1))\n\u001B[0;32m     31\u001B[0m     l3 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(l2))\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from utils import plotLearning\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    print(env.observation_space.shape[0])\n",
    "    agent = Agent(gamma=.99,\n",
    "                  initial_eps=1.0,\n",
    "                  eps_decay=2e-5,\n",
    "                  final_eps=0.05,\n",
    "                  batch_size=128,\n",
    "                  n_actions=4,\n",
    "                  input_dims=[8],\n",
    "                  lr=0.0001,\n",
    "                  max_mem_size=10_000)\n",
    "    scores, eps_history = [],[]\n",
    "    n_episodes = 1000\n",
    "    j= n_episodes\n",
    "    avg_score =-9999\n",
    "    for i in range(n_episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        # if avg_score >-40 :\n",
    "        #     j=i\n",
    "        #     break\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs_, reward, terminated, truncated, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(obs, action, reward, obs_, done)\n",
    "            agent.learn()\n",
    "            obs = obs_\n",
    "            done = terminated or truncated\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.eps)\n",
    "        #\n",
    "        if i % 100 == 0:\n",
    "                avg_score = np.mean(scores[-100:]) \n",
    "                print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.eps) )\n",
    "    x = [i+1 for i in range(j)]\n",
    "    filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores,eps_history, filename)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:21:06.734463700Z",
     "start_time": "2024-08-09T01:09:28.621868200Z"
    }
   },
   "id": "cf1c9f9629526b19",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('LunarLander-v2', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['nothing','Left','main','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 1000\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:21:06.740970500Z",
     "start_time": "2024-08-09T01:21:06.735967600Z"
    }
   },
   "id": "73f443e038177892",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
