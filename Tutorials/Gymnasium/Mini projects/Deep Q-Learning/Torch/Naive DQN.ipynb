{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:12:17.591537600Z",
     "start_time": "2024-08-04T10:12:17.583534400Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:12:17.611584600Z",
     "start_time": "2024-08-04T10:12:17.598575100Z"
    }
   },
   "id": "57dc8ea63dd65481",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 256)\n",
    "        self.fc2 = nn.Linear(256,n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1)\n",
    "        \n",
    "        return actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:12:17.626112500Z",
     "start_time": "2024-08-04T10:12:17.616606600Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 n_actions,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.001,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=1e-4,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = T.tensor(obs, dtype=T.float).to(self.Q.device)\n",
    "                actions = self.Q.forward(state)\n",
    "                action = T.argmax(actions).item()\n",
    "                # print('All Actions: ', actions)\n",
    "                # print('Selected Action: ', action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        state = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        action = T.tensor(action).to(self.Q.device)\n",
    "        reward = T.tensor(reward).to(self.Q.device)\n",
    "        next_state = T.tensor(next_state, dtype=T.float).to(self.Q.device)\n",
    "        \n",
    "        \n",
    "        # print(f'states {states.shape}, next_states {next_states.shape}')\n",
    "        q_pred = self.Q.forward(state)[action]\n",
    "        # print(f'q_pred: {q_pred}')\n",
    "        q_next = self.Q.forward(next_state).max()\n",
    "#         print(f'q_next_max: {q_next}')        \n",
    "        q_target = reward + self.gamma * q_next\n",
    "#         print(f'q_target: {q_target}')\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "            \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:12:17.646479600Z",
     "start_time": "2024-08-04T10:12:17.631617500Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score 10.0 avg score 10.0 epsilon 0.999\n",
      "Episode 100 score 13.0 avg score 21.3 epsilon 0.786\n",
      "Episode 200 score 12.0 avg score 21.9 epsilon 0.566\n",
      "Episode 300 score 17.0 avg score 24.7 epsilon 0.320\n",
      "Episode 400 score 15.0 avg score 29.5 epsilon 0.025\n",
      "Episode 500 score 47.0 avg score 47.6 epsilon 0.010\n",
      "Episode 600 score 32.0 avg score 49.5 epsilon 0.010\n",
      "Episode 700 score 58.0 avg score 52.7 epsilon 0.010\n",
      "Episode 800 score 40.0 avg score 50.7 epsilon 0.010\n",
      "Episode 900 score 44.0 avg score 43.2 epsilon 0.010\n",
      "Episode 1000 score 46.0 avg score 44.9 epsilon 0.010\n",
      "Episode 1100 score 52.0 avg score 46.1 epsilon 0.010\n",
      "Episode 1200 score 28.0 avg score 43.7 epsilon 0.010\n",
      "Episode 1300 score 58.0 avg score 44.8 epsilon 0.010\n",
      "Episode 1400 score 30.0 avg score 44.3 epsilon 0.010\n",
      "Episode 1500 score 59.0 avg score 46.3 epsilon 0.010\n",
      "Episode 1600 score 38.0 avg score 55.6 epsilon 0.010\n",
      "Episode 1700 score 92.0 avg score 45.7 epsilon 0.010\n",
      "Episode 1800 score 46.0 avg score 50.7 epsilon 0.010\n",
      "Episode 1900 score 98.0 avg score 46.9 epsilon 0.010\n",
      "Episode 2000 score 67.0 avg score 49.0 epsilon 0.010\n",
      "Episode 2100 score 73.0 avg score 54.2 epsilon 0.010\n",
      "Episode 2200 score 90.0 avg score 57.2 epsilon 0.010\n",
      "Episode 2300 score 39.0 avg score 48.6 epsilon 0.010\n",
      "Episode 2400 score 52.0 avg score 51.5 epsilon 0.010\n",
      "Episode 2500 score 67.0 avg score 47.7 epsilon 0.010\n",
      "Episode 2600 score 23.0 avg score 44.4 epsilon 0.010\n",
      "Episode 2700 score 59.0 avg score 43.2 epsilon 0.010\n",
      "Episode 2800 score 97.0 avg score 50.9 epsilon 0.010\n",
      "Episode 2900 score 39.0 avg score 59.3 epsilon 0.010\n",
      "Episode 3000 score 66.0 avg score 91.8 epsilon 0.010\n",
      "Episode 3100 score 500.0 avg score 225.7 epsilon 0.010\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    nb_episodes = 10000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    agent = Agent(input_dims= env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  lr=0.0001\n",
    "                  )\n",
    "    avg_score = 0\n",
    "    for i in range (nb_episodes):\n",
    "        if avg_score > 100 : \n",
    "            break\n",
    "        else:\n",
    "            score = 0\n",
    "            done = False\n",
    "            obs, _ = env.reset()\n",
    "            while not done:\n",
    "                action = agent.choose_action(obs)\n",
    "                next_obs, reward, terminated,truncated, _ = env.step(action)\n",
    "                score+= reward\n",
    "                agent.learn(obs, action, reward, next_obs)\n",
    "                obs = next_obs\n",
    "                done = truncated or terminated\n",
    "            scores.append(score)\n",
    "            eps_history.append(agent.epsilon)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                \n",
    "                print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.epsilon) )\n",
    "        \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:19:27.209645800Z",
     "start_time": "2024-08-04T10:12:17.654987600Z"
    }
   },
   "id": "a7cbe0fb29d7d5fc",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "can't invoke \"update\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTclError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[51], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m action_text\u001B[38;5;241m.\u001B[39mset_text(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStep: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactions[action]\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m fig\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mdraw()\n\u001B[1;32m---> 26\u001B[0m \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[0;32m     28\u001B[0m obs \u001B[38;5;241m=\u001B[39m next_obs\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:414\u001B[0m, in \u001B[0;36mFigureCanvasTk.flush_events\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflush_events\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;66;03m# docstring inherited\u001B[39;00m\n\u001B[1;32m--> 414\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tkcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py:1314\u001B[0m, in \u001B[0;36mMisc.update\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Enter event loop until all pending events have been processed by Tcl.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1314\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mupdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTclError\u001B[0m: can't invoke \"update\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['Left','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 10\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:32:55.358130200Z",
     "start_time": "2024-08-04T10:32:40.384683300Z"
    }
   },
   "id": "ecfedeba4554ff4d",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T10:19:39.704574200Z",
     "start_time": "2024-08-04T10:19:39.703574600Z"
    }
   },
   "id": "7ec8c5d01b740702",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
