{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:28:48.193201200Z",
     "start_time": "2024-07-28T02:28:48.177122300Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:28:48.207274900Z",
     "start_time": "2024-07-28T02:28:48.202271900Z"
    }
   },
   "id": "57dc8ea63dd65481",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128,n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1)\n",
    "        \n",
    "        return actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:28:48.229813100Z",
     "start_time": "2024-07-28T02:28:48.218796600Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 n_actions,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.0001,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=1e-5,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = T.tensor(obs, dtype=T.float).to(self.Q.device)\n",
    "                actions = self.Q.forward(state)\n",
    "                action = T.argmax(actions).item()\n",
    "                # print('All Actions: ', actions)\n",
    "                # print('Selected Action: ', action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        state = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        action = T.tensor(action).to(self.Q.device)\n",
    "        reward = T.tensor(reward).to(self.Q.device)\n",
    "        next_state = T.tensor(next_state, dtype=T.float).to(self.Q.device)\n",
    "        \n",
    "        \n",
    "        # print(f'states {states.shape}, next_states {next_states.shape}')\n",
    "        q_pred = self.Q.forward(state)[action]\n",
    "        # print(f'q_pred: {q_pred}')\n",
    "        q_next = self.Q.forward(next_state).max()\n",
    "#         print(f'q_next_max: {q_next}')        \n",
    "        q_target = reward + self.gamma * q_next\n",
    "#         print(f'q_target: {q_target}')\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "            \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:28:48.254636100Z",
     "start_time": "2024-07-28T02:28:48.238607300Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score 10.0 avg score 10.0 epsilon 1.000\n",
      "Episode 100 score 32.0 avg score 23.5 epsilon 0.976\n",
      "Episode 200 score 16.0 avg score 23.9 epsilon 0.953\n",
      "Episode 300 score 21.0 avg score 21.6 epsilon 0.931\n",
      "Episode 400 score 15.0 avg score 24.5 epsilon 0.906\n",
      "Episode 500 score 53.0 avg score 22.9 epsilon 0.884\n",
      "Episode 600 score 20.0 avg score 23.2 epsilon 0.860\n",
      "Episode 700 score 65.0 avg score 25.1 epsilon 0.835\n",
      "Episode 800 score 13.0 avg score 24.6 epsilon 0.811\n",
      "Episode 900 score 28.0 avg score 27.7 epsilon 0.783\n",
      "Episode 1000 score 19.0 avg score 25.2 epsilon 0.758\n",
      "Episode 1100 score 12.0 avg score 28.4 epsilon 0.729\n",
      "Episode 1200 score 65.0 avg score 30.0 epsilon 0.699\n",
      "Episode 1300 score 45.0 avg score 28.5 epsilon 0.671\n",
      "Episode 1400 score 17.0 avg score 36.3 epsilon 0.635\n",
      "Episode 1500 score 12.0 avg score 37.2 epsilon 0.597\n",
      "Episode 1600 score 11.0 avg score 32.8 epsilon 0.565\n",
      "Episode 1700 score 54.0 avg score 36.5 epsilon 0.528\n",
      "Episode 1800 score 19.0 avg score 33.9 epsilon 0.494\n",
      "Episode 1900 score 60.0 avg score 35.5 epsilon 0.459\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1')\n",
    "    nb_episodes = 2000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    agent = Agent(input_dims= env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  lr=0.001\n",
    "                  )\n",
    "    \n",
    "    for i in range (nb_episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs, _ = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            next_obs, reward, terminated,truncated, _ = env.step(action)\n",
    "            score+= reward\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            done = truncated or terminated\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.epsilon) )\n",
    "    \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:31:24.153384Z",
     "start_time": "2024-07-28T02:28:48.255636100Z"
    }
   },
   "id": "a7cbe0fb29d7d5fc",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['Left','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 10\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecfedeba4554ff4d",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:32:32.097333600Z",
     "start_time": "2024-07-28T02:32:30.728353700Z"
    }
   },
   "id": "7ec8c5d01b740702",
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
