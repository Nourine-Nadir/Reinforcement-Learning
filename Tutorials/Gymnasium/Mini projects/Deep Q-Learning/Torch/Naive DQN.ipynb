{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-27T21:21:02.223361700Z",
     "start_time": "2024-07-27T21:21:02.190516800Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T21:21:02.224380600Z",
     "start_time": "2024-07-27T21:21:02.208787300Z"
    }
   },
   "id": "57dc8ea63dd65481",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        layer1 = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(layer1)\n",
    "        \n",
    "        return actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T21:21:02.253211100Z",
     "start_time": "2024-07-27T21:21:02.223361700Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 n_actions,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.1,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=0.9995,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = T.tensor(obs, dtype=T.float).to(self.Q.device)\n",
    "                actions = self.Q.forward(state)\n",
    "                action = T.argmax(actions).item()\n",
    "                # print('All Actions: ', actions)\n",
    "                # print('Selected Action: ', action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        self.Q.optimizer.zero_grad()\n",
    "        state = T.tensor(state, dtype=T.float).to(self.Q.device)\n",
    "        action = T.tensor(action).to(self.Q.device)\n",
    "        reward = T.tensor(reward).to(self.Q.device)\n",
    "        next_state = T.tensor(next_state, dtype=T.float).to(self.Q.device)\n",
    "        \n",
    "        \n",
    "        # print(f'states {states.shape}, next_states {next_states.shape}')\n",
    "        q_pred = self.Q.forward(state)[action]\n",
    "        # print(f'q_pred: {q_pred}')\n",
    "        q_next = self.Q.forward(next_state).max()\n",
    "#         print(f'q_next_max: {q_next}')        \n",
    "        q_target = reward + self.gamma * q_next\n",
    "#         print(f'q_target: {q_target}')\n",
    "        loss = self.Q.loss(q_target, q_pred).to(self.Q.device)\n",
    "        loss.backward()\n",
    "        self.Q.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "            \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T21:21:02.254230Z",
     "start_time": "2024-07-27T21:21:02.244655800Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score 21.0 avg score 21.0 epsilon 0.990\n",
      "Episode 100 score 11.0 avg score 23.8 epsilon 0.301\n",
      "Episode 200 score 45.0 avg score 34.7 epsilon 0.053\n",
      "Episode 300 score 27.0 avg score 33.7 epsilon 0.010\n",
      "Episode 400 score 9.0 avg score 40.6 epsilon 0.010\n",
      "Episode 500 score 40.0 avg score 45.2 epsilon 0.010\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v1',max_episode_steps=150)\n",
    "    nb_episodes = 3000\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    agent = Agent(input_dims= env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  lr=0.001\n",
    "                  )\n",
    "    \n",
    "    for i in range (nb_episodes):\n",
    "        score = 0\n",
    "        done = False\n",
    "        obs, _ = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            next_obs, reward, terminated,truncated, _ = env.step(action)\n",
    "            score+= reward\n",
    "            agent.learn(obs, action, reward, next_obs)\n",
    "            obs = next_obs\n",
    "            done = truncated or terminated\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('Episode', i, 'score %.1f avg score %.1f epsilon %.3f'% (score, avg_score,agent.epsilon) )\n",
    "    \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-27T21:21:02.256643800Z"
    }
   },
   "id": "a7cbe0fb29d7d5fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7ec8c5d01b740702",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
