{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:04:05.025537800Z",
     "start_time": "2024-07-19T18:04:05.001038300Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 initial_epsilon: float,\n",
    "                 epsilon_decay: float,\n",
    "                 final_epsilon: float,\n",
    "                 discount_factor: float = 0.95,\n",
    "                 discrete_actions: int = 6):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.discrete_actions = discrete_actions\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.discrete_actions))\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        # Round each value in the state to 1 decimal place\n",
    "        # Convert to tuple for hashability\n",
    "       \n",
    "          # Slice to exclude last element\n",
    "        # Append the original terminated flag (boolean)\n",
    "        return tuple(np.append(state, state[-1])) \n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.discrete_actions)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[discretized_state]))\n",
    "    \n",
    "    def update_q_values(self, state, action, reward, terminated, next_state):\n",
    "        state = self.discretize_state(state)\n",
    "        next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        if not terminated:          \n",
    "            future_q_value = np.max(self.q_values[next_state])\n",
    "        else:\n",
    "            future_q_value = 0\n",
    "        temporal_difference = (reward + (self.discount_factor * future_q_value)) - self.q_values[state][action]\n",
    "        self.q_values[state][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:04:05.049170Z",
     "start_time": "2024-07-19T18:04:05.034069Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "n_episodes = 10_000\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.99\n",
    "final_epsilon = 0.05\n",
    "\n",
    "agent = PongAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:04:05.077725600Z",
     "start_time": "2024-07-19T18:04:05.060211600Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-ramNoFrameskip-v0\", render_mode='rgb_array')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:04:05.187912200Z",
     "start_time": "2024-07-19T18:04:05.092751100Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:03<00:00, 54.48it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=60)\n",
    "\n",
    "rewards = 0 \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        # update the agent\n",
    "        agent.update_q_values(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:07:08.773798600Z",
     "start_time": "2024-07-19T18:04:05.205939400Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "q_values = np.array([value for key, value in agent.q_values.items()])\n",
    "print(np.argmax(q_values,axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:07:08.804245700Z",
     "start_time": "2024-07-19T18:07:08.791105Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:07:08.835746800Z",
     "start_time": "2024-07-19T18:07:08.821227300Z"
    }
   },
   "id": "7c08f6b5a0cde63e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Click\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001B[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:  obs = [192   0   0   0 110  38   0   7 131   1   0   0  70   0   0  63 255   0\n",
      " 255 253   0 130   0  24 128  32   1  86 247  86 247  86 247 134 243 245\n",
      " 243 240 240 242 242  32  32  64  64  64 188  65 189 126 130 109  37  37\n",
      " 130   0   1   0   1 109 109  37  37 192 192 192 192   1 192 202 247 202\n",
      " 247 202 247 202 247   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  53  85  54 236 242\n",
      " 121 240] , reward = 0.0\n",
      "step 0:  obs = [192   0   0   0 110  38   0   7  71   2   0  63  14   1   0  63 255   1\n",
      "   0   2   0 162   0  24 128  32   1  86 247  86 247  86 247 134 243 245\n",
      " 243 240 240 242 242  32  32  64  64  64 188  65 189 205 162  67  37  37\n",
      " 175   0   2   0 255 109  67  37  37 192 192 192 192 192 192 207 247 202\n",
      " 247 202 247 202 247   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 180  85  54 236 242\n",
      " 121 240] , reward = -1.0\n",
      "step 0:  obs = [192   0   0   0 110  38   0   7 211   2   0  63  14   2   0  63 255   0\n",
      "   0   2   0 204   0  24 128  32   1  86 247  86 247  86 247 134 243 245\n",
      " 243 240 240 242 242  32  32  64  64  64 188  65 189 205 204  85  37  37\n",
      " 205   0   1   0 255 109  85  37  37 192 192 192 192 192 192 212 247 202\n",
      " 247 202 247 202 247   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 180  85  54 236 242\n",
      " 121 240] , reward = -1.0\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "can't invoke \"update\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTclError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m action_text\u001B[38;5;241m.\u001B[39mset_text(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStep: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[action]\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     29\u001B[0m fig\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mdraw()\n\u001B[1;32m---> 30\u001B[0m \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[0;32m     32\u001B[0m obs \u001B[38;5;241m=\u001B[39m next_obs\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:414\u001B[0m, in \u001B[0;36mFigureCanvasTk.flush_events\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflush_events\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;66;03m# docstring inherited\u001B[39;00m\n\u001B[1;32m--> 414\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tkcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py:1314\u001B[0m, in \u001B[0;36mMisc.update\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Enter event loop until all pending events have been processed by Tcl.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1314\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mupdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTclError\u001B[0m: can't invoke \"update\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Pong-ramNoFrameskip-v0\",render_mode='rgb_array')\n",
    "# env = CustomRewardWrapper(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "img = ax.imshow(env.render())\n",
    "actions = ['Move Up','Move Right','Move Down','Move Left']\n",
    "rewards = 0\n",
    "num_epochs= 1\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if reward < 0 : \n",
    "            print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:07:38.927165700Z",
     "start_time": "2024-07-19T18:07:08.851853700Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'mean episode rewards = {rewards/num_epochs}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T18:07:38.951659400Z",
     "start_time": "2024-07-19T18:07:38.944410700Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "print(f'observation space shape : {env.observation_space}') \n",
    "# print(f'observation space numbers : {env.nS}') \n",
    "#-------------- obesrvation is a tupe of 3 values : --------------\n",
    "#1) player cards value\n",
    "#2) dealer's face up card\n",
    "#3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "\n",
    "print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# print(f'\\nEnv specs : {env.spec}') \n",
    "print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-19T18:07:38.959660600Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
