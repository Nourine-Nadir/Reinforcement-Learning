{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# →Q-Learning\n",
    "\n",
    "It is temporal difference learning on $Q$-function\n",
    "\n",
    "$Q^{new}(s_k,a_k)=Q^{old}(s_k,a_k)+\\alpha(r_k+\\gamma \\max_a Q(s_{k+1},a)-Q^{old}(s_k,a_k))$ \n",
    "\n",
    "Off policy TD(0) learning of the quality function Q\n",
    "\n",
    "What we mean by **Off policy** is that we can take **sub-optimal** $a_k$ actions to get the reward but still **maximize** the next action in $s_{k+1}$ though, this helps to learn even when **not taking best** $a_k$ actions.\n",
    "\n",
    "The Off policy can be **confusing** since we are saying that we can take **sub-optimal** actions but there is that **term** in the update function: $max_a Q(s_{k+1},a)$\n",
    "\n",
    "**Many** policies are used in **experiments** and at the **experience replay** step we iterate through actions even if they are sub-optimal but we **assume** that the **best** actions will be taken in next steps. This is done by replaying experiments done **by us** or **importing** others and learn from them; this ensure treating **different** policies.\n",
    "\n",
    "**Exploration vs. exploitation: $\\epsilon$-greedy actions**\n",
    "\n",
    "**Random** exploration element is introduced to $Q$-learning, the popular technique is the  **$\\epsilon$-greedy.** Taking the action $a_k$ will be taken based on the current $Q$ function, with a probability $1-\\epsilon$, where $\\epsilon \\in[0,1]$. for example $\\epsilon=0.05$ there will be a 95% **probability** of taking best action and 5% **chance** of exploring a sub-optimal one. \n",
    "\n",
    "This epsilon value can be decayed as we iterate to go more **On-Policy** once we learned a good $Q$-function.\n",
    "\n",
    "$Q$ -learning applies to **discrete** action spaces $A$ and state spaces $S$ governed by a **finite** MDP. A table of $Q$ values is used to represent the $Q$ function, and thus it doesn’t **scale** well to **large** state spaces. Typically function **approximation** is used to represent the $Q$ function, such as a **neural network** in deep $Q$-learning.\n",
    "\n",
    "> Because $Q$-learning is off-policy, it is possible to learn from action-state sequences that do not use the current optimal policy. For example, it is possible to store past experiences, such as previously played games, and replay these experiences to further improve the Q function.\n",
    ">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a921405e4837cf78"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:53.465350600Z",
     "start_time": "2024-07-20T11:43:52.894736Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomFrozenLake(FrozenLakeEnv):\n",
    "    def __init__(self, goal_reward=100, hole_penalty=-50, step_penalty=-1,stuck_penalty=-1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.goal_reward = goal_reward\n",
    "        self.hole_penalty = hole_penalty\n",
    "        self.step_penalty = step_penalty\n",
    "        self.stuck_penalty = stuck_penalty\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_state = self.unwrapped.s\n",
    "        \n",
    "        state, reward, terminated, truncated, info = super().step(action)\n",
    "        \n",
    "        current_tile = self.desc[self.unwrapped.s // self.ncol, self.unwrapped.s % self.ncol]\n",
    "        \n",
    "        if current_tile in b'H':\n",
    "            reward = self.hole_penalty  # Apply penalty for falling into a hole\n",
    "        elif current_tile in b'G':\n",
    "            reward = self.goal_reward  # Apply higher reward for reaching the goal\n",
    "        \n",
    "        elif prev_state == state :\n",
    "            reward = self.stuck_penalty  # Apply small penalty for walking on frozen tiles\n",
    "        else:\n",
    "            reward = self.step_penalty\n",
    "            \n",
    "        \n",
    "        return state, reward, terminated, truncated, info"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:53.474729100Z",
     "start_time": "2024-07-20T11:43:53.466350800Z"
    }
   },
   "id": "45654de722988832",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FrozenLakeAgent():\n",
    "    def __init__(self,\n",
    "                 learning_rate:float,\n",
    "                 initial_epsilon:float,\n",
    "                 epsilon_decay:float,\n",
    "                 final_epsilon:float,\n",
    "                 discount_factor:float = 0.95,\n",
    "                 ):\n",
    "        \n",
    "    #Initialize the agent with empty dictionary of action/state values (q_values), a learning rate and an epsilon\n",
    "    # discount_factor : Is for computing the Q-value namely gamma \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def choose_action(self, obs:tuple[int,int,bool])->int:\n",
    "        # Return the best action with a probability of (1- epsilon) \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    \n",
    "    def update_q_values(self,\n",
    "                        obs:tuple[int,int,bool],\n",
    "                        action:int,\n",
    "                        reward:float,\n",
    "                        terminated:bool,\n",
    "                        next_obs:tuple[int,int,bool]):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "\n",
    "        temporal_diffrence = (reward + (self.discount_factor * future_q_value))- self.q_values[obs][action]\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_diffrence\n",
    "        )\n",
    "        self.training_error.append(temporal_diffrence)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:53.554689600Z",
     "start_time": "2024-07-20T11:43:53.475729Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "n_episodes = 10000\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.999\n",
    "final_epsilon = 0.05\n",
    "\n",
    "agent = FrozenLakeAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:53.612128200Z",
     "start_time": "2024-07-20T11:43:53.554689600Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'24x24'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mCustomFrozenLake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmap_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m24x24\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_slippery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrender_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m, in \u001B[0;36mCustomFrozenLake.__init__\u001B[1;34m(self, goal_reward, hole_penalty, step_penalty, stuck_penalty, **kwargs)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, goal_reward\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, hole_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m50\u001B[39m, step_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,stuck_penalty\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgoal_reward \u001B[38;5;241m=\u001B[39m goal_reward\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhole_penalty \u001B[38;5;241m=\u001B[39m hole_penalty\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:228\u001B[0m, in \u001B[0;36mFrozenLakeEnv.__init__\u001B[1;34m(self, render_mode, desc, map_name, is_slippery)\u001B[0m\n\u001B[0;32m    226\u001B[0m     desc \u001B[38;5;241m=\u001B[39m generate_random_map()\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m desc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 228\u001B[0m     desc \u001B[38;5;241m=\u001B[39m \u001B[43mMAPS\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmap_name\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdesc \u001B[38;5;241m=\u001B[39m desc \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(desc, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnrow, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mncol \u001B[38;5;241m=\u001B[39m nrow, ncol \u001B[38;5;241m=\u001B[39m desc\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[1;31mKeyError\u001B[0m: '24x24'"
     ]
    }
   ],
   "source": [
    "env = CustomFrozenLake(map_name=\"24x24\", is_slippery=False, render_mode='rgb_array')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:44:25.261718400Z",
     "start_time": "2024-07-20T11:44:25.216840Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=60)\n",
    "\n",
    "rewards = 0 \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        # update the agent\n",
    "        agent.update_q_values(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-20T11:43:54.319893600Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q_values = np.array([value for key, value in agent.q_values.items()])\n",
    "print(np.argmax(q_values,axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-20T11:43:54.322430Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Blackjack environment, the state space is defined by three components:\n",
    "\n",
    "- The player's current sum (ranges from 4 to 21)\n",
    "- The dealer's visible card (ranges from 1 to 10, where 1 represents an Ace)\n",
    "- Whether the player has a usable Ace (True or False)\n",
    "\n",
    "So, the total number of possible states is:\n",
    "(21 - 4 + 1) * 10 * 2 = 18 * 10 * 2 = 360\n",
    "However, you're seeing 380 instead of 360. This is because the environment also includes some terminal states that can occur when the player's sum exceeds 21 (bust states). These additional states account for the extra 20 entries in your q_values dictionary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1c59375e9d58b82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:54.326723600Z",
     "start_time": "2024-07-20T11:43:54.325723Z"
    }
   },
   "id": "29b323d24413638a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'total rewards = {rewards}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-20T11:43:54.327723700Z"
    }
   },
   "id": "7c08f6b5a0cde63e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "# env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "img = ax.imshow(env.render())\n",
    "actions = ['Move Up','Move Right','Move Down','Move Left']\n",
    "rewards = 0\n",
    "num_epochs= 3\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "        print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(.05)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-20T11:43:54.329724Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'total rewards = {rewards}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-20T11:43:54.339299300Z",
     "start_time": "2024-07-20T11:43:54.330724600Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "# print(f'observation space shape : {env.observation_space}') \n",
    "# #-------------- obesrvation is a tupe of 3 values : --------------\n",
    "# #1) player cards value\n",
    "# #2) dealer's face up card\n",
    "# #3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "# \n",
    "# print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# # print(f'\\nEnv specs : {env.spec}') \n",
    "# print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-20T11:43:54.410223400Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
