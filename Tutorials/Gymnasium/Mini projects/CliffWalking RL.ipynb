{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:14.591073Z",
     "start_time": "2024-07-19T20:01:14.119967200Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CliffWalkingAgent():\n",
    "    def __init__(self,\n",
    "                 learning_rate:float,\n",
    "                 initial_epsilon:float,\n",
    "                 epsilon_decay:float,\n",
    "                 final_epsilon:float,\n",
    "                 discount_factor:float = 0.95,\n",
    "                 ):\n",
    "        \n",
    "    #Initialize the agent with empty dictionary of action/state values (q_values), a learning rate and an epsilon\n",
    "    # discount_factor : Is for computing the Q-value namely gamma \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def choose_action(self, obs:tuple[int,int,bool])->int:\n",
    "        # Return the best action with a probability of (1- epsilon) \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    \n",
    "    def update_q_values(self,\n",
    "                        obs:tuple[int,int,bool],\n",
    "                        action:int,\n",
    "                        reward:float,\n",
    "                        terminated:bool,\n",
    "                        next_obs:tuple[int,int,bool]):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "\n",
    "        temporal_diffrence = (reward + (self.discount_factor * future_q_value))- self.q_values[obs][action]\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_diffrence\n",
    "        )\n",
    "        self.training_error.append(temporal_diffrence)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:14.600074800Z",
     "start_time": "2024-07-19T20:01:14.582071600Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_episodes = 10_000\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.99\n",
    "final_epsilon = 0.08\n",
    "\n",
    "agent = CliffWalkingAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:15.812346900Z",
     "start_time": "2024-07-19T20:01:14.600074800Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0', render_mode='rgb_array')\n",
    "env.reward_range = (-100, 100)\n",
    "def custom_reward(observation, reward, done, info):\n",
    "    if observation == env.goal_state:\n",
    "        reward = 100  # Set reward of 10 for reaching the goal\n",
    "    return observation, reward, done, info\n",
    "\n",
    "env.reward_wrapper = custom_reward "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:18.273899900Z",
     "start_time": "2024-07-19T20:01:15.813347100Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:07<00:00, 1341.88it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=60)\n",
    "\n",
    "rewards = 0 \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        # update the agent\n",
    "        agent.update_q_values(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:26.287294700Z",
     "start_time": "2024-07-19T20:01:18.211885600Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 2 1 1 2 1 1 2 1 1 2 1 1 1 0 2 3 1 1 1 2 1 1 2 1 2 1 1 1 1 1 1 2 2 2\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "q_values = np.array([value for key, value in agent.q_values.items()])\n",
    "print(np.argmax(q_values,axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:26.316838500Z",
     "start_time": "2024-07-19T20:01:26.290294200Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = -469126\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:26.773201500Z",
     "start_time": "2024-07-19T20:01:26.318838200Z"
    }
   },
   "id": "7c08f6b5a0cde63e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:  obs = 24 , reward = -1\n",
      "step 0:  obs = 25 , reward = -1\n",
      "step 0:  obs = 26 , reward = -1\n",
      "step 0:  obs = 27 , reward = -1\n",
      "step 0:  obs = 28 , reward = -1\n",
      "step 0:  obs = 29 , reward = -1\n",
      "step 0:  obs = 30 , reward = -1\n",
      "step 0:  obs = 31 , reward = -1\n",
      "step 0:  obs = 32 , reward = -1\n",
      "step 0:  obs = 33 , reward = -1\n",
      "step 0:  obs = 34 , reward = -1\n",
      "step 0:  obs = 35 , reward = -1\n",
      "Goal reached! Observation: 47\n",
      "step 0:  obs = 47 , reward = 10\n",
      "step 1:  obs = 24 , reward = -1\n",
      "step 1:  obs = 25 , reward = -1\n",
      "step 1:  obs = 26 , reward = -1\n",
      "step 1:  obs = 27 , reward = -1\n",
      "step 1:  obs = 28 , reward = -1\n",
      "step 1:  obs = 29 , reward = -1\n",
      "step 1:  obs = 36 , reward = -100\n",
      "step 1:  obs = 36 , reward = -1\n",
      "step 1:  obs = 24 , reward = -1\n",
      "step 1:  obs = 25 , reward = -1\n",
      "step 1:  obs = 26 , reward = -1\n",
      "step 1:  obs = 27 , reward = -1\n",
      "step 1:  obs = 28 , reward = -1\n",
      "step 1:  obs = 29 , reward = -1\n",
      "step 1:  obs = 30 , reward = -1\n",
      "step 1:  obs = 31 , reward = -1\n",
      "step 1:  obs = 32 , reward = -1\n",
      "step 1:  obs = 33 , reward = -1\n",
      "step 1:  obs = 34 , reward = -1\n",
      "step 1:  obs = 35 , reward = -1\n",
      "Goal reached! Observation: 47\n",
      "step 1:  obs = 47 , reward = 10\n",
      "step 2:  obs = 24 , reward = -1\n",
      "step 2:  obs = 25 , reward = -1\n",
      "step 2:  obs = 26 , reward = -1\n",
      "step 2:  obs = 36 , reward = -100\n",
      "step 2:  obs = 24 , reward = -1\n",
      "step 2:  obs = 25 , reward = -1\n",
      "step 2:  obs = 26 , reward = -1\n",
      "step 2:  obs = 27 , reward = -1\n",
      "step 2:  obs = 28 , reward = -1\n",
      "step 2:  obs = 29 , reward = -1\n",
      "step 2:  obs = 30 , reward = -1\n",
      "step 2:  obs = 31 , reward = -1\n",
      "step 2:  obs = 32 , reward = -1\n",
      "step 2:  obs = 33 , reward = -1\n",
      "step 2:  obs = 34 , reward = -1\n",
      "step 2:  obs = 35 , reward = -1\n",
      "Goal reached! Observation: 47\n",
      "step 2:  obs = 47 , reward = 10\n",
      "step 3:  obs = 24 , reward = -1\n",
      "step 3:  obs = 25 , reward = -1\n",
      "step 3:  obs = 26 , reward = -1\n",
      "step 3:  obs = 27 , reward = -1\n",
      "step 3:  obs = 28 , reward = -1\n",
      "step 3:  obs = 29 , reward = -1\n",
      "step 3:  obs = 30 , reward = -1\n",
      "step 3:  obs = 31 , reward = -1\n",
      "step 3:  obs = 32 , reward = -1\n",
      "step 3:  obs = 33 , reward = -1\n",
      "step 3:  obs = 34 , reward = -1\n",
      "step 3:  obs = 35 , reward = -1\n",
      "Goal reached! Observation: 47\n",
      "step 3:  obs = 47 , reward = 10\n",
      "step 4:  obs = 24 , reward = -1\n",
      "step 4:  obs = 25 , reward = -1\n",
      "step 4:  obs = 26 , reward = -1\n",
      "step 4:  obs = 27 , reward = -1\n",
      "step 4:  obs = 28 , reward = -1\n",
      "step 4:  obs = 29 , reward = -1\n",
      "step 4:  obs = 30 , reward = -1\n",
      "step 4:  obs = 31 , reward = -1\n",
      "step 4:  obs = 32 , reward = -1\n",
      "step 4:  obs = 33 , reward = -1\n",
      "step 4:  obs = 34 , reward = -1\n",
      "step 4:  obs = 35 , reward = -1\n",
      "Goal reached! Observation: 47\n",
      "step 4:  obs = 47 , reward = 10\n"
     ]
    }
   ],
   "source": [
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.goal_reward = 10  # Set desired reward for reaching the goal\n",
    "        self.goal_state = 47  # The actual goal state for CliffWalking\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Check if the agent has reached the goal state\n",
    "        if observation == self.goal_state:\n",
    "            print(f'Goal reached! Observation: {observation}')\n",
    "            reward = self.goal_reward\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('CliffWalking-v0',render_mode='rgb_array')\n",
    "env = CustomRewardWrapper(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "img = ax.imshow(env.render())\n",
    "actions = ['Move Up','Move Right','Move Down','Move Left']\n",
    "rewards = 0\n",
    "num_epochs= 3\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "        print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(.05)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:02:54.094647800Z",
     "start_time": "2024-07-19T20:02:45.377898600Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = -6\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:02:45.369009700Z",
     "start_time": "2024-07-19T20:02:45.360012Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space shape : 4\n",
      "observation space shape : Discrete(48)\n",
      "observation space numbers : 48\n",
      "reward range : (-inf, inf)\n",
      "\n",
      "Env metadata : {'render_modes': ['human', 'rgb_array', 'ansi'], 'render_fps': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Click\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001B[33mWARN: env.nS to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.nS` for environment variables or `env.get_wrapper_attr('nS')` that will search the reminding wrappers.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "print(f'observation space shape : {env.observation_space}') \n",
    "print(f'observation space numbers : {env.nS}') \n",
    "#-------------- obesrvation is a tupe of 3 values : --------------\n",
    "#1) player cards value\n",
    "#2) dealer's face up card\n",
    "#3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "\n",
    "print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# print(f'\\nEnv specs : {env.spec}') \n",
    "print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:02:45.377898600Z",
     "start_time": "2024-07-19T20:02:45.366009400Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
