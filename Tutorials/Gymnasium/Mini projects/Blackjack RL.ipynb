{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:17.569967700Z",
     "start_time": "2024-07-19T20:06:17.364666200Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BlackjackAgent():\n",
    "    def __init__(self,\n",
    "                 learning_rate:float,\n",
    "                 initial_epsilon:float,\n",
    "                 epsilon_decay:float,\n",
    "                 final_epsilon:float,\n",
    "                 discount_factor:float = 0.95,\n",
    "                 ):\n",
    "        \n",
    "    #Initialize the agent with empty dictionary of action/state values (q_values), a learning rate and an epsilon\n",
    "    # discount_factor : Is for computing the Q-value namely gamma \n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def choose_action(self, obs:tuple[int,int,bool])->int:\n",
    "        # Return the best action with a probability of (1- epsilon) \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    \n",
    "    def update_q_values(self,\n",
    "                        obs:tuple[int,int,bool],\n",
    "                        action:int,\n",
    "                        reward:float,\n",
    "                        terminated:bool,\n",
    "                        next_obs:tuple[int,int,bool]):\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "\n",
    "        temporal_diffrence = (reward + (self.discount_factor * future_q_value))- self.q_values[obs][action]\n",
    "        \n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_diffrence\n",
    "        )\n",
    "        self.training_error.append(temporal_diffrence)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:17.572491800Z",
     "start_time": "2024-07-19T20:06:17.556459900Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 100_00\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.80\n",
    "final_epsilon = 0.05\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:17.686123900Z",
     "start_time": "2024-07-19T20:06:17.584379100Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:17.738229200Z",
     "start_time": "2024-07-19T20:06:17.677596400Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 5322.98it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "rewards = 0 \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        # update the agent\n",
    "        agent.update_q_values(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:19.748066Z",
     "start_time": "2024-07-19T20:06:17.757261300Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n"
     ]
    }
   ],
   "source": [
    "print(len(agent.q_values))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:19.750066Z",
     "start_time": "2024-07-19T20:06:19.725009300Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Blackjack environment, the state space is defined by three components:\n",
    "\n",
    "- The player's current sum (ranges from 4 to 21)\n",
    "- The dealer's visible card (ranges from 1 to 10, where 1 represents an Ace)\n",
    "- Whether the player has a usable Ace (True or False)\n",
    "\n",
    "So, the total number of possible states is:\n",
    "(21 - 4 + 1) * 10 * 2 = 18 * 10 * 2 = 360\n",
    "However, you're seeing 380 instead of 360. This is because the environment also includes some terminal states that can occur when the player's sum exceeds 21 (bust states). These additional states account for the extra 20 entries in your q_values dictionary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1c59375e9d58b82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:20.007504500Z",
     "start_time": "2024-07-19T20:06:19.758230200Z"
    }
   },
   "id": "29b323d24413638a",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = -1009.0\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:20.146345700Z",
     "start_time": "2024-07-19T20:06:20.138823800Z"
    }
   },
   "id": "7c08f6b5a0cde63e",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:  obs = (13, 5, 1)\n",
      "step 0:  obs = (21, 5, 1)\n",
      "step 1:  obs = (13, 3, 0)\n",
      "step 1:  obs = (15, 3, 0)\n",
      "step 2:  obs = (20, 8, 0)\n",
      "step 3:  obs = (12, 5, 0)\n",
      "step 3:  obs = (20, 5, 0)\n",
      "step 4:  obs = (12, 7, 0)\n",
      "step 4:  obs = (19, 7, 0)\n",
      "step 5:  obs = (13, 2, 0)\n",
      "step 6:  obs = (17, 8, 0)\n",
      "step 7:  obs = (15, 10, 1)\n",
      "step 8:  obs = (21, 1, 1)\n",
      "step 9:  obs = (17, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['Stick','Hit']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 10\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        print(f'step {step}:  obs = {obs}')\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(.5)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:24.401338400Z",
     "start_time": "2024-07-19T20:06:20.159091900Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = 2.0\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:24.442379900Z",
     "start_time": "2024-07-19T20:06:24.417426900Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:24.466072500Z",
     "start_time": "2024-07-19T20:06:24.451166700Z"
    }
   },
   "id": "fb8534ec66ebbc84",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space shape : 2\n",
      "observation space shape : Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "reward range : (-inf, inf)\n",
      "\n",
      "Env metadata : {'render_modes': ['human', 'rgb_array'], 'render_fps': 4}\n"
     ]
    }
   ],
   "source": [
    "print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "print(f'observation space shape : {env.observation_space}') \n",
    "#-------------- obesrvation is a tupe of 3 values : --------------\n",
    "#1) player cards value\n",
    "#2) dealer's face up card\n",
    "#3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "\n",
    "print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# print(f'\\nEnv specs : {env.spec}') \n",
    "print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:06:25.314430900Z",
     "start_time": "2024-07-19T20:06:24.481087300Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
