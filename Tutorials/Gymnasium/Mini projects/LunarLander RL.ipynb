{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:11.992490100Z",
     "start_time": "2024-07-19T20:01:11.513382100Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict # allows access to undefined keys\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LunarLanderAgent:\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 initial_epsilon: float,\n",
    "                 epsilon_decay: float,\n",
    "                 final_epsilon: float,\n",
    "                 discount_factor: float = 0.95,\n",
    "                 discrete_actions: int = 4):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.discrete_actions = discrete_actions\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.discrete_actions))\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        # Round each value in the state to 1 decimal place\n",
    "        # Convert to tuple for hashability\n",
    "       \n",
    "        rounded_state = np.round(state, 1)  # Slice to exclude last element\n",
    "        # Append the original terminated flag (boolean)\n",
    "        return tuple(np.append(rounded_state, state[-1])) \n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.discrete_actions)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[discretized_state]))\n",
    "    \n",
    "    def update_q_values(self, state, action, reward, terminated, next_state):\n",
    "        state = self.discretize_state(state)\n",
    "        next_state = self.discretize_state(next_state)\n",
    "        \n",
    "        if not terminated:          \n",
    "            future_q_value = np.max(self.q_values[next_state])\n",
    "        else:\n",
    "            future_q_value = 0\n",
    "        temporal_difference = (reward + (self.discount_factor * future_q_value)) - self.q_values[state][action]\n",
    "        self.q_values[state][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:12.009493Z",
     "start_time": "2024-07-19T20:01:11.997491200Z"
    }
   },
   "id": "71382b775326e544",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "n_episodes = 100_00\n",
    "start_epsilon = 1\n",
    "epsilon_decay = 0.999\n",
    "final_epsilon = 0.05\n",
    "\n",
    "agent = LunarLanderAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    \n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:12.190533900Z",
     "start_time": "2024-07-19T20:01:12.010493500Z"
    }
   },
   "id": "ef93af5919d22a69",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:01:19.430161100Z",
     "start_time": "2024-07-19T20:01:12.186532900Z"
    }
   },
   "id": "40360a309c0449ea",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:47<00:00, 93.17it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=60)\n",
    "\n",
    "rewards = 0 \n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        # update the agent\n",
    "        agent.update_q_values(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:03:06.779330800Z",
     "start_time": "2024-07-19T20:01:19.425159300Z"
    }
   },
   "id": "f66e0e3d2ae896c4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "q_values = np.array([value for key, value in agent.q_values.items()])\n",
    "print(np.argmax(q_values,axis=1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:03:06.811083500Z",
     "start_time": "2024-07-19T20:03:06.781834200Z"
    }
   },
   "id": "fa5d6e44bd8378ae",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards = -225771.23506791456\n"
     ]
    }
   ],
   "source": [
    "print(f'total rewards = {rewards}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:03:06.825580300Z",
     "start_time": "2024-07-19T20:03:06.811586100Z"
    }
   },
   "id": "7c08f6b5a0cde63e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:  obs = [ 0.31820184  1.0875041   0.30053353 -0.41212827  0.37081236  0.32681724\n",
      "  0.          0.        ] , reward = 0.7095849306204058\n",
      "step 1:  obs = [-0.16466483 -0.00452212 -0.06644684 -0.28559577  0.57780063 -3.7748184\n",
      "  0.          0.        ] , reward = -100\n",
      "step 2:  obs = [ 8.22419152e-02  1.59128048e-02 -3.84804085e-02  1.71540456e-03\n",
      "  2.29143596e+00  1.15736976e-01  0.00000000e+00  1.00000000e+00] , reward = -100\n",
      "step 2:  obs = [ 0.14008875  0.02842799 -0.49921054 -1.1228324   1.8841434   4.565578\n",
      "  0.          1.        ] , reward = 35.696896095282796\n",
      "step 3:  obs = [ 0.13448295  0.00990977 -0.39614922  0.0747238   2.0831795   1.236011\n",
      "  0.          0.        ] , reward = -100\n",
      "step 4:  obs = [-0.05357037  0.00526329  0.47250214 -0.18862799 -1.2630514  -5.0811567\n",
      "  1.          0.        ] , reward = -100\n",
      "step 5:  obs = [-0.3058497  -0.04613066 -0.4588543  -0.20637606 -1.637465    1.226955\n",
      "  1.          0.        ] , reward = -100\n",
      "step 5:  obs = [-0.17261915  0.04614781  0.4012693  -0.41760662  0.7260236   2.0436854\n",
      "  0.          1.        ] , reward = 89.1876946017286\n",
      "step 5:  obs = [-0.16345826  0.03316413  0.45011577 -0.21412566  0.76885766 -0.73498183\n",
      "  0.          1.        ] , reward = 13.499534663120158\n",
      "step 6:  obs = [-0.12741347 -0.0289156   0.47896796 -0.28623584  0.16556452 -2.0209227\n",
      "  1.          0.        ] , reward = -100\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "can't invoke \"update\" command: application has been destroyed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTclError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 30\u001B[0m\n\u001B[0;32m     27\u001B[0m action_text\u001B[38;5;241m.\u001B[39mset_text(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStep: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactions[action]\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     29\u001B[0m fig\u001B[38;5;241m.\u001B[39mcanvas\u001B[38;5;241m.\u001B[39mdraw()\n\u001B[1;32m---> 30\u001B[0m \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush_events\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m done \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n\u001B[0;32m     32\u001B[0m obs \u001B[38;5;241m=\u001B[39m next_obs\n",
      "File \u001B[1;32m~\\PycharmProjects\\DeepLearning\\.venv\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:414\u001B[0m, in \u001B[0;36mFigureCanvasTk.flush_events\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflush_events\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;66;03m# docstring inherited\u001B[39;00m\n\u001B[1;32m--> 414\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tkcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tkinter\\__init__.py:1314\u001B[0m, in \u001B[0;36mMisc.update\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1313\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Enter event loop until all pending events have been processed by Tcl.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1314\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mupdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTclError\u001B[0m: can't invoke \"update\" command: application has been destroyed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"LunarLander-v2\",render_mode='rgb_array')\n",
    "# env = CustomRewardWrapper(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "img = ax.imshow(env.render())\n",
    "actions = ['Move Up','Move Right','Move Down','Move Left']\n",
    "rewards = 0\n",
    "num_epochs= 3\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if reward >10 : \n",
    "            print(f'step {step}:  obs = {next_obs} , reward = {reward}')\n",
    "\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T20:04:05.938748900Z",
     "start_time": "2024-07-19T20:03:06.829581400Z"
    }
   },
   "id": "795a6112a7a42623",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'mean episode rewards = {rewards/num_epochs}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-19T20:04:05.933747800Z"
    }
   },
   "id": "26a84eafbb54162e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'action space shape : {env.action_space.n}') # Number of possible actions is 4\n",
    "print(f'observation space shape : {env.observation_space}') \n",
    "print(f'observation space numbers : {env.nS}') \n",
    "#-------------- obesrvation is a tupe of 3 values : --------------\n",
    "#1) player cards value\n",
    "#2) dealer's face up card\n",
    "#3) usable ace for player, equal 1 if ace is considered an 11 without busting\n",
    "\n",
    "print(f'reward range : {env.reward_range}') # default reward range is set to -inf +inf\n",
    "# print(f'\\nEnv specs : {env.spec}') \n",
    "print(f'\\nEnv metadata : {env.metadata}') # render_modes adn render_fps"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-19T20:04:05.935748800Z"
    }
   },
   "id": "c44a122356184824",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
