{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:23:04.437064700Z",
     "start_time": "2024-09-02T13:23:04.429399700Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:23:04.466637200Z",
     "start_time": "2024-09-02T13:23:04.444573300Z"
    }
   },
   "id": "948a80f02f2787d1",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LinearDeepQNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 lr,\n",
    "                 n_actions,\n",
    "                 input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = layers.Dense(128, activation='relu', input_shape=input_dims)\n",
    "        self.fc2 = layers.Dense( n_actions, activation=None)\n",
    "        self.flatten = layers.Flatten() \n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss = keras.losses.MeanSquaredError()\n",
    "    \n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        with tf.device('/GPU:0'):  \n",
    "\n",
    "            x = self.fc1(inputs)\n",
    "            x = self.fc2(x)\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:23:04.466637200Z",
     "start_time": "2024-09-02T13:23:04.461090900Z"
    }
   },
   "id": "6e305f56b3d419c6",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 input_dims,\n",
    "                 n_actions,\n",
    "                 gamma=0.99,\n",
    "                 lr=0.0001,\n",
    "                 initial_epsilon=1,\n",
    "                 epsilon_decay=1e-4,\n",
    "                 final_epsilon=0.01):\n",
    "        self.lr = lr\n",
    "        self.n_actions = n_actions\n",
    "        self.input_dims = input_dims\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.Q = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "            if np.random.random() > self.epsilon:\n",
    "                state = tf.convert_to_tensor([obs], dtype=tf.float32)\n",
    "                actions = self.Q(state)\n",
    "                actions = tf.squeeze(actions)\n",
    "                action = tf.argmax(actions)\n",
    "                # print('All Actions: ', actions)\n",
    "                # print('Selected Action: ', action.numpy())\n",
    "                action = int(action.numpy())\n",
    "            else:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            states = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor([action], dtype=tf.int32)\n",
    "            rewards = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "            \n",
    "            # print(f'states {states.shape}, next_states {next_states.shape}')\n",
    "            q_pred = self.Q.call(states)\n",
    "#             print(f'q_pred: {q_pred.shape}')\n",
    "            q_next = tf.reduce_max(self.Q.call(next_states))\n",
    "            q_next_max = tf.reduce_max(q_next)\n",
    "            # print(f'q_next_max: {q_next_max.shape}')        \n",
    "            q_target = rewards + self.gamma * q_next_max\n",
    "            # print(f'q_target: {q_target.shape}')\n",
    "            loss = self.Q.loss(q_pred, q_target)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.Q.trainable_weights)\n",
    "        self.Q.optimizer.apply_gradients(zip(gradients, self.Q.trainable_weights))\n",
    "        self.decrement_epsilon()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:23:04.489663Z",
     "start_time": "2024-09-02T13:23:04.481156400Z"
    }
   },
   "id": "3a06b59178cb0b87",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0  avg score 27.0 epsilon 0.997\n",
      "Episode 100  avg score 21.4 epsilon 0.783\n",
      "Episode 200  avg score 22.8 epsilon 0.555\n",
      "Episode 300  avg score 22.7 epsilon 0.328\n",
      "Episode 400  avg score 24.7 epsilon 0.081\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    nb_episodes = 500 \n",
    "    env = gym.make('CartPole-v1')\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=nb_episodes)\n",
    "    scores = []\n",
    "    eps_history = []\n",
    "    agent = Agent(input_dims= env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  lr=0.0001\n",
    "                  )\n",
    "    avg_score =0\n",
    "    for i in range (nb_episodes):\n",
    "        if avg_score > 100 : \n",
    "            break\n",
    "        else:\n",
    "            score = 0\n",
    "            done = False\n",
    "            obs, _ = env.reset()\n",
    "            while not done:\n",
    "                action = agent.choose_action(obs)\n",
    "                next_obs, reward, terminated,truncated, _ = env.step(action)\n",
    "                score+= reward\n",
    "                agent.learn(obs, action, reward, next_obs)\n",
    "                obs = next_obs\n",
    "                done = truncated or terminated\n",
    "            scores.append(score)\n",
    "            eps_history.append(agent.epsilon)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                \n",
    "                print('Episode', i, ' avg score %.1f epsilon %.3f'% ( avg_score,agent.epsilon) )\n",
    "        \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:25:30.745980300Z",
     "start_time": "2024-09-02T13:23:04.498183300Z"
    }
   },
   "id": "a7cbe0fb29d7d5fc",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "rolling_length = 20\n",
    "plt.title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "plt.plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "figs_dir = \"figs\"\n",
    "if not os.path.exists(figs_dir):\n",
    "    os.makedirs(figs_dir)\n",
    "\n",
    "# Save the plot to the \"figs\" directory\n",
    "plt.savefig(os.path.join(figs_dir, \"rewards.png\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:25:30.953178100Z",
     "start_time": "2024-09-02T13:25:30.759488300Z"
    }
   },
   "id": "7ec8c5d01b740702",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg' if you prefer Qt\n",
    "env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n",
    "obs, info = env.reset()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "action_text = ax.text(510, 20, '', color='white', fontsize=12, bbox=dict(facecolor='blue', alpha=0.8))\n",
    "actions = ['Left','Right']\n",
    "img = ax.imshow(env.render())\n",
    "rewards = 0\n",
    "num_epochs= 10\n",
    "for step in range(num_epochs):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        rewards += reward\n",
    "        frame = env.render()\n",
    "        img.set_data(frame)\n",
    "        action_text.set_text(f'Step: {actions[action] }')\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "plt.ioff()  # Turn off interactive mode\n",
    "# plt.show()  # Keep the window open after the animation finishes\n",
    "plt.close()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T13:25:55.438438500Z",
     "start_time": "2024-09-02T13:25:31.046138200Z"
    }
   },
   "id": "bcf0b9dfbeacb01c",
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
